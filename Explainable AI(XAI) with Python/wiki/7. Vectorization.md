## Text Vectorization
자연어 처리에서는 기계가 문자를 이해 할 수 있도록 수치화해주는 과정이 반드시 필요하다. 다음은 텍스트 벡터화의 대표적인 방법 세 가지이다.

1. **원-핫 인코딩(One-hot encoding)**
원-핫 인코딩은 텍스트를 유의미한 숫자(벡터)로 바꾸는 가장 쉬운 방법이다. N 개의 단어를 각각 N 차원의 벡터로 표현하는 방식이다. 단어에 해당되는 차원(인덱스)에 1을 넣고 나머지에는 0을 넣는다. 원-핫 인코딩은 단어 또는 문자(characters)를 기준으로 벡터화 할 수 있다. 다음은 원-핫 인코딩을 이용한 단어 벡터화의 예이다.

예) `The cat sat on the mat.`

원-핫 벡터의 특징은 다음과 같다.


![](https://icim.nims.re.kr/file/817c4d05ba5c41958a3a05c56913d300.png)
- 하나의 요소만 1이고 나머지는 모두 0인 희소 벡터(sparse vector)이다.
- 원-핫 벡터의 차원은 말뭉치(corpus) 내 단어의 수와 같다.
- 두 원-핫 벡터 간의 내적(inner product)이 0이다. 즉, - 두 벡터는 직교(orthogonal)이며 서로 독립(independent)이다.
- 단어의 특정한 관계나 의미를 포함하지 않는다.

2. **TF-IDF: 빈도수 기반 텍스트(문서) 벡터화 방법**

TF-IDF(Term Frequency - Inverse Document Frequency)는 특정 단어가 문서 내에 등장하는 빈도(TF: 단어빈도)와 그 단어가 문서 전체 집합에서 등장하는 빈도(IDF: 역문서 빈도)를 고려하여 벡터화 하는 방법이다. 하나의 문서 단위로 벡터를 만들며 각 인덱스에 해당하는 단어가 문서에 등장하는 빈도와 문서 집합 전체에 등장하는 빈도의 역수를 곱하여 구하게 된다.

3. **단어 임베딩(Word Embedding)**

단어를 벡터화하는 또 다른 방법으로 분산 표상(Distributedsimilarity based representation) 개념을 바탕으로 의미를 포함하는 단어 벡터로 바꾸는 단어 임베딩 기법이 있다. 이는 비슷한 분포를 가진 단어의 주변 단어들도 비슷한 의미를 가진다는 것을 가정한다. 원-핫 인코딩과는 달리, 분산 표상에서는 하나의 단어가 미리 정한 차원(200~300차원 정도)에서 연속형의 값을 갖는 벡터로 표현된다. 이렇게 만들어진 단어 벡터는 단어의 의미 담고있으며, 단어 벡터 간의 연산도 가능하다.
![](https://icim.nims.re.kr/file/83c48e65fff34e1eaa77241afe722c74.png)

많은 양의 문서를 학습하여 얻어진 단어 벡터는 단어 간의 관계를 보다 정확하게 나타낸다. 단어를 벡터로 임베딩하는 방식은 머신러닝을 통해 학습된다. 신경망을 기반으로 한 단어 벡터화의 대표적 방법은 Word2Vec이 있다. Word2vec은 CBOW(continuous bag of words)와 Skip-gram(SG)의 두 가지 알고리즘이 있고 일반적으로 SG가 CBOW 알고리즘보다 학습이 잘 이루어진다고 알려져 있다. 그 외 GloVe, FastText과 같은 방법론도 있다.

