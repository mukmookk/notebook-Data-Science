## Understanding Metrics to know quality of Classifier

- The quality of classifier is measured in terms of True Positive, False Positive, True Negative and False Negative, Precision, Recall and F-Measure.

## True Positive

- Those instances where predicted class and actual class are both positive are called as true positive. True positive is an outcome where the model correctly predicts the positive class.
- For example in case of bird classifier, the bird that are correctly identified as birds are called as true positive.

## True Negative

- Those instances where predicted class and actual class are both negative are called as true negative. True negative is an outcome where the model correctly predicts the negative class.
- For example, in case of our bird classifier there are images that are not of birds which our classifier correctly identified as "not a bird" are called as true negatives.

## False Positive

- Those instances where predicted class is positive, but actually the instance are negative. False positice is an outcome where the model incorrectly predicts the positive class.
- For example, in case of our bird classifier there are some images that the classifier predicted as birds but they were something else. These are our false positives.

## False Negative

- Those instances where predicted class is negative, but actually the instance are positive. False negative is an outcome where the model incorrectly predicts the negative class.
- For example, in case of our bird classifier there are some images of birds that the classifier did not correctly recognize as birds. These are our false negatives.

## Confusion matrix

- Confusion matirx is a NxN table that summarize the accuracy of a classification model's predictions. Here, N represents the number of classes. In a binary classification problem, N=2.
- In simple words, it is a correlation between the actual labels and the model's prediction labels. One axis of a confusion matrix is the label the model predicted, and the other axis is the actual label.

## Precision

- Precision identifies the frequency with which a model is correct when prediciting the positive class.

- We can say that precision <=> True Positives / All Positive predicted.
- `P = TP / (TP + FP)`
- In other words, if we predict positive then how often was it really a positive instance.

## Recall

- Recall identifies out of all the possible positive labels, how many did the model correctly identify?
- It refers to what percentage of actual positive instances we are able to find.
- So, Recall = True Positive / All actual positive instances
- `R = TP / (TP + FN)`

## F-Measure

- In statistical analysis of binary classification, the F-Measure (also F-score or F-1 score) is a measure of a test's accuracy. It considers both the precision `p` and tge recall `r` of the test to compute the score.
- `F = 2 * (precision * recall) / (precision + recall)
- The `F1 score` is the harmonic average of the precision and recall, where F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0.
  