## Local Interpretable Model Agnostic Explanations

**Local** => Scope is local used to explain single observation or record.

**Interpretable** => Human understandable expalination of the model.

**Model Agnostic Explanations** => It can be used to explain any black box model.

## LIME: Local Interpretable Model Agnostic Explainations.

- Lime is aim to reduce distance between AI and humans.

- LIME is a people-oriented approach.
- It focuses on two main areas:
  - Trust the model
  - Trusting the predictions

## Sample dataset: LIME Approval Prediction

Feature Descirption
|Variables|Description|
|---------|-----------|
|Loan_ID|Unique Loan ID|
|Gender| Male/Female|
|Married| Applicant married (Y/N)|
|Dependents| Number of dependents|
|Education| Applicant Education (Graduate/Under Graduate)|
|Self_Employed| Self employed(Y/N)|
|ApplicantIncome| Applicant Income|
|CoapplicantIncome | Coapplicant Income|
|LoanAmount | Loan amount in thousands|
|Loan_Amount_Term| Term of loan in months|
|Credit_History| credit histoty meets guidelines|
|Property_Area| Urban / Semi Urban / Rural|
|Loan_Status| Loan approved (Y/N)|

## LIME over Loan Dataset

- A black box algorithm has been trained on two features, `Applicant_Income` and `Loan_Amount`. After the training the decision boundary of our complex model is as shown in figure.

- This figure is highly non-linear and uninterpretable, because its not easy to explain why model is making a particular type of prediction.

- Let's suppose our AI sysem had rejected the loan to him.
- The question is how could we explain to him why our model had rejected his application.
- **Solution is LIME**: Zoom into the instance and build a simple model around it.
- Create a simple explaination that makes sense in that local region.
- No need to worry about the rest of the model. Get a Valid explanation for selected instance.
- We can explain value of `Loan_Amount_Term` was small enough to reject the loan.
- It also tells that changes in `Applicant_Income` almost have no impact in the output.
- It also explains the importance of our variables, as `Loan_Amount_Term` here mainly determines our output class.
- LIME fits a linear interpretable model in local area, also called surrogate. A local approximation to our complex model.

## LIME Developers

Ribeiro, Maco Tulio, Sameer Singh, and Carlos Guestrin. **"Why shoud i trust you?" Expalining the predictions of any classifier** 

Proceeding of the 22nd ACM SIGKDD international conference on knowledge discorvery and data mining. 2016 6000+ Citiations.

## LIME mian features

- Works on any black box model
- Internals are hidden make explanations by just observing inputs and outputs.
- Locally faithful and may not globally.
- It can work with many data types.
- Using domain knowledge we can build the trust on AI model and can even make corrections in the model

**Example**
We know that `Applicant_Income` has a positive impact on the chances of loan approval if the LIME explanation tells us that it increases the chance of a rejection of loan then there must be something wrong in our developed model. So, it helps to build trust and improve model.

## Mathmathical Modeling of LIME

$$
E(x) = \underset{g\in G}{\mathrm{argmin}} \ L(f, g, ∏x) + Ω(g)
$$

- `x`: features (Married, Applicant_Income, LoanAmount, Loan_Amount_Term, ...)
- `f`: Complex model
- `g`: Simple interpretable model
- `∏x`: Proximity
- `L(f, g, ∏x)`: Good approximation
</br>
- `Ω(g)`: Stay simple
</br>
- `G`: Family of interpretable models Like `Regression`, `Decision Tree`.

For linear regression for instance a desirable condition could be to have many zero-weight input features.
This helps to ignore most of the features and just including a few to makes our explanations simpler.

So, overall this omega is a complexity measure and as this optimization problem is a minimization problem.

**simple Glassbox 1: Simple Linear Regression**
$$
y = b_0 + b_1x1
$$

**Multiple Linear Regression**
$$
y = b_0 + b_1x1 + b+2x_2 + b_3x_3 + ... + b_nx_n 
$$

**Example**
$$
Y' = b_0 + b_1x1 + b+2x_2
$$

$$
Y` = 32.3 + (0.01)Married + (0.8)Applicant_Income +(0.02)Gender + (-0.8)LoanAmount + (0.98 Loan_Amount_Term) + (0)City
$$

**Important Feature**: `Applicant_Income`, `LoanAmount`, `LoantAmopunt_Term`
**Positive for Loan Approval**: `Applicant_Income`, `Loan_Amount_Term`
**Negative for Loan Approval**: `LoanAmount`
**Not Important**: `City`, `Gender`, `Married`

**simple Glass box 2: Decision Tree**
----

Lets apply LIME in case Rahat (Two step Process)

**Step-1**: Generate Random Datapoints in neighborhood of input data point

We simply generate some new data points in the neighborhood of our input data point.

For this we randomly generate data points everywhere.

Input data points are generated by deviations. We can slightly increase the `Applicant Income` and `Loan_Amount_Term`

**Step-2**: Get the prediction for these data points using our complex model `f`


> New dataset
> *Label*: Prediction of complex model.
> *Features*: Newly generated datapoints.

We got labels data by applying complex model on new data points. Then we train linear model on this labelled data for selected features.

- For a linear regression we minimize the sum of square distance between the predictions and the actual values.
- Here, ground truth or actual values comes from the complex model `f` and the predicted values from the prediction made by simple model `g`.
- The proximity `pi` is added to weight the loss according to how close data point.